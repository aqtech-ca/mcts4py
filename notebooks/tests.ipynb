{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted state value: 0.09520085965602287\n"
     ]
    }
   ],
   "source": [
    "from mcts4py.QValueEstimator import *\n",
    "\n",
    "# Example usage:\n",
    "state_dim = 10\n",
    "action_dim = 5\n",
    "hidden_dim = 50\n",
    "q_estimator = QValueEstimator(state_dim, action_dim, hidden_dim)\n",
    "\n",
    "# Random initialization of the networks\n",
    "# def initialize_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.uniform_(m.weight, -0.5, 0.5)\n",
    "#         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# q_estimator.state_value_net.apply(initialize_weights)\n",
    "# q_estimator.q_net.apply(initialize_weights)\n",
    "\n",
    "# Predict state value\n",
    "state = np.random.randn(state_dim)\n",
    "state_value = q_estimator.get_state_value(state)\n",
    "print(\"Predicted state value:\", state_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted state-action value: 0.6086512036331215\n"
     ]
    }
   ],
   "source": [
    "# Predict state-action value\n",
    "action = np.random.randint(0, action_dim, size=action_dim)\n",
    "state_action_value = q_estimator.get_q_value(state, action)\n",
    "print(\"Predicted state-action value:\", state_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Update state-action value\n",
    "target_state_action_value = np.random.randn(1)\n",
    "q_estimator.update_q_value(state, action, target_state_action_value, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19907922,  0.57904742, -0.27053891, -0.44541629,  0.51236063])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_values = np.random.randn(action_dim)\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7942445121749403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions = [[0, 1, 2, 3, 4], [0, 1, 5, 3, 5]]\n",
    "q_estimator.get_max_q_value(state, possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 5, 3, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_softmax_prob_per_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/mcts4py/mcts4py/QValueEstimator.py:105\u001b[0m, in \u001b[0;36mQValueEstimator.get_softmax_prob_per_action\u001b[0;34m(self, state, action, lambda_temp_term)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     decay_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: lambda_temp_term\n\u001b[0;32m--> 105\u001b[0m decay_term \u001b[38;5;241m=\u001b[39m \u001b[43mdecay_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_calls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m prob_ments \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m decay_term)\u001b[38;5;241m*\u001b[39mrho_ments \u001b[38;5;241m+\u001b[39m decay_term\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space_size\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob_ments\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "q_estimator.get_softmax_prob_per_action(state, possible_actions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-0.2538941468193494: 0.5,\n",
       " -0.10414975209228486: 0.5,\n",
       " -1.5548246230013554: 0.5,\n",
       " 1.4599978532288413: 0.5,\n",
       " 1.269592037777111: 0.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_estimator.get_softmax_prob_multinom(state, action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5548246230013554"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_estimator.draw_from_multinomial(q_estimator.get_softmax_prob_multinom(state, action_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = 0.590000, f(x) = 5.808100\n",
      "Decayed learning rate to 0.099000\n",
      "Iteration 2: x = 1.057379, f(x) = 3.773776\n",
      "Decayed learning rate to 0.098010\n",
      "Iteration 3: x = 1.428566, f(x) = 2.469406\n",
      "Decayed learning rate to 0.097030\n",
      "Iteration 4: x = 1.724103, f(x) = 1.627913\n",
      "Decayed learning rate to 0.096060\n",
      "Iteration 5: x = 1.960000, f(x) = 1.081600\n",
      "Decayed learning rate to 0.095099\n",
      "Iteration 6: x = 2.148762, f(x) = 0.724606\n",
      "Decayed learning rate to 0.094148\n",
      "Iteration 7: x = 2.300183, f(x) = 0.489744\n",
      "Decayed learning rate to 0.093207\n",
      "Iteration 8: x = 2.421951, f(x) = 0.334141\n",
      "Decayed learning rate to 0.092274\n",
      "Iteration 9: x = 2.520114, f(x) = 0.230290\n",
      "Decayed learning rate to 0.091352\n",
      "Iteration 10: x = 2.599446, f(x) = 0.160444\n",
      "Decayed learning rate to 0.090438\n",
      "Iteration 11: x = 2.663718, f(x) = 0.113086\n",
      "Decayed learning rate to 0.089534\n",
      "Iteration 12: x = 2.715919, f(x) = 0.080702\n",
      "Decayed learning rate to 0.088638\n",
      "Iteration 13: x = 2.758423, f(x) = 0.058359\n",
      "Decayed learning rate to 0.087752\n",
      "Iteration 14: x = 2.793120, f(x) = 0.042799\n",
      "Decayed learning rate to 0.086875\n",
      "Iteration 15: x = 2.821518, f(x) = 0.031856\n",
      "Decayed learning rate to 0.086006\n",
      "Iteration 16: x = 2.844822, f(x) = 0.024080\n",
      "Decayed learning rate to 0.085146\n",
      "Iteration 17: x = 2.863998, f(x) = 0.018497\n",
      "Decayed learning rate to 0.084294\n",
      "Iteration 18: x = 2.879821, f(x) = 0.014443\n",
      "Decayed learning rate to 0.083451\n",
      "Iteration 19: x = 2.892915, f(x) = 0.011467\n",
      "Decayed learning rate to 0.082617\n",
      "Iteration 20: x = 2.903783, f(x) = 0.009258\n",
      "Decayed learning rate to 0.081791\n",
      "Iteration 21: x = 2.912833, f(x) = 0.007598\n",
      "Decayed learning rate to 0.080973\n",
      "Iteration 22: x = 2.920393, f(x) = 0.006337\n",
      "Decayed learning rate to 0.080163\n",
      "Iteration 23: x = 2.926730, f(x) = 0.005369\n",
      "Decayed learning rate to 0.079361\n",
      "Iteration 24: x = 2.932061, f(x) = 0.004616\n",
      "Decayed learning rate to 0.078568\n",
      "Iteration 25: x = 2.936564, f(x) = 0.004024\n",
      "Decayed learning rate to 0.077782\n",
      "Iteration 26: x = 2.940382, f(x) = 0.003554\n",
      "Decayed learning rate to 0.077004\n",
      "Iteration 27: x = 2.943634, f(x) = 0.003177\n",
      "Decayed learning rate to 0.076234\n",
      "Iteration 28: x = 2.946417, f(x) = 0.002871\n",
      "Decayed learning rate to 0.075472\n",
      "Iteration 29: x = 2.948809, f(x) = 0.002621\n",
      "Decayed learning rate to 0.074717\n",
      "Iteration 30: x = 2.950876, f(x) = 0.002413\n",
      "Decayed learning rate to 0.073970\n",
      "Iteration 31: x = 2.952672, f(x) = 0.002240\n",
      "Decayed learning rate to 0.073230\n",
      "Iteration 32: x = 2.954241, f(x) = 0.002094\n",
      "Decayed learning rate to 0.072498\n",
      "Iteration 33: x = 2.955620, f(x) = 0.001970\n",
      "Decayed learning rate to 0.071773\n",
      "Iteration 34: x = 2.956839, f(x) = 0.001863\n",
      "Decayed learning rate to 0.071055\n",
      "Iteration 35: x = 2.957924, f(x) = 0.001770\n",
      "Decayed learning rate to 0.070345\n",
      "Iteration 36: x = 2.958895, f(x) = 0.001690\n",
      "Decayed learning rate to 0.069641\n",
      "Iteration 37: x = 2.959770, f(x) = 0.001618\n",
      "Decayed learning rate to 0.068945\n",
      "Iteration 38: x = 2.960564, f(x) = 0.001555\n",
      "Decayed learning rate to 0.068255\n",
      "Iteration 39: x = 2.961289, f(x) = 0.001499\n",
      "Decayed learning rate to 0.067573\n",
      "Iteration 40: x = 2.961954, f(x) = 0.001447\n",
      "Decayed learning rate to 0.066897\n",
      "Iteration 41: x = 2.962569, f(x) = 0.001401\n",
      "Decayed learning rate to 0.066228\n",
      "Iteration 42: x = 2.963141, f(x) = 0.001359\n",
      "Decayed learning rate to 0.065566\n",
      "Iteration 43: x = 2.963676, f(x) = 0.001319\n",
      "Decayed learning rate to 0.064910\n",
      "Iteration 44: x = 2.964178, f(x) = 0.001283\n",
      "Decayed learning rate to 0.064261\n",
      "Iteration 45: x = 2.964652, f(x) = 0.001249\n",
      "Decayed learning rate to 0.063619\n",
      "Iteration 46: x = 2.965103, f(x) = 0.001218\n",
      "Decayed learning rate to 0.062982\n",
      "Iteration 47: x = 2.965532, f(x) = 0.001188\n",
      "Decayed learning rate to 0.062353\n",
      "Iteration 48: x = 2.965942, f(x) = 0.001160\n",
      "Decayed learning rate to 0.061729\n",
      "Iteration 49: x = 2.966336, f(x) = 0.001133\n",
      "Decayed learning rate to 0.061112\n",
      "Iteration 50: x = 2.966716, f(x) = 0.001108\n",
      "Decayed learning rate to 0.060501\n",
      "Iteration 51: x = 2.967083, f(x) = 0.001084\n",
      "Decayed learning rate to 0.059896\n",
      "Iteration 52: x = 2.967439, f(x) = 0.001060\n",
      "Decayed learning rate to 0.059297\n",
      "Iteration 53: x = 2.967784, f(x) = 0.001038\n",
      "Decayed learning rate to 0.058704\n",
      "Iteration 54: x = 2.968121, f(x) = 0.001016\n",
      "Decayed learning rate to 0.058117\n",
      "Iteration 55: x = 2.968449, f(x) = 0.000995\n",
      "Decayed learning rate to 0.057535\n",
      "Iteration 56: x = 2.968769, f(x) = 0.000975\n",
      "Decayed learning rate to 0.056960\n",
      "Iteration 57: x = 2.969082, f(x) = 0.000956\n",
      "Decayed learning rate to 0.056391\n",
      "Iteration 58: x = 2.969389, f(x) = 0.000937\n",
      "Decayed learning rate to 0.055827\n",
      "Iteration 59: x = 2.969690, f(x) = 0.000919\n",
      "Decayed learning rate to 0.055268\n",
      "Iteration 60: x = 2.969986, f(x) = 0.000901\n",
      "Decayed learning rate to 0.054716\n",
      "Iteration 61: x = 2.970277, f(x) = 0.000883\n",
      "Decayed learning rate to 0.054169\n",
      "Iteration 62: x = 2.970563, f(x) = 0.000867\n",
      "Decayed learning rate to 0.053627\n",
      "Iteration 63: x = 2.970844, f(x) = 0.000850\n",
      "Decayed learning rate to 0.053091\n",
      "Iteration 64: x = 2.971121, f(x) = 0.000834\n",
      "Decayed learning rate to 0.052560\n",
      "Iteration 65: x = 2.971395, f(x) = 0.000818\n",
      "Decayed learning rate to 0.052034\n",
      "Iteration 66: x = 2.971664, f(x) = 0.000803\n",
      "Decayed learning rate to 0.051514\n",
      "Iteration 67: x = 2.971930, f(x) = 0.000788\n",
      "Decayed learning rate to 0.050999\n",
      "Iteration 68: x = 2.972192, f(x) = 0.000773\n",
      "Decayed learning rate to 0.050489\n",
      "Iteration 69: x = 2.972451, f(x) = 0.000759\n",
      "Decayed learning rate to 0.049984\n",
      "Iteration 70: x = 2.972706, f(x) = 0.000745\n",
      "Decayed learning rate to 0.049484\n",
      "Iteration 71: x = 2.972959, f(x) = 0.000731\n",
      "Decayed learning rate to 0.048989\n",
      "Iteration 72: x = 2.973208, f(x) = 0.000718\n",
      "Decayed learning rate to 0.048499\n",
      "Iteration 73: x = 2.973455, f(x) = 0.000705\n",
      "Decayed learning rate to 0.048014\n",
      "Iteration 74: x = 2.973699, f(x) = 0.000692\n",
      "Decayed learning rate to 0.047534\n",
      "Iteration 75: x = 2.973940, f(x) = 0.000679\n",
      "Decayed learning rate to 0.047059\n",
      "Iteration 76: x = 2.974178, f(x) = 0.000667\n",
      "Decayed learning rate to 0.046588\n",
      "Iteration 77: x = 2.974413, f(x) = 0.000655\n",
      "Decayed learning rate to 0.046122\n",
      "Iteration 78: x = 2.974646, f(x) = 0.000643\n",
      "Decayed learning rate to 0.045661\n",
      "Iteration 79: x = 2.974877, f(x) = 0.000631\n",
      "Decayed learning rate to 0.045204\n",
      "Iteration 80: x = 2.975105, f(x) = 0.000620\n",
      "Decayed learning rate to 0.044752\n",
      "Iteration 81: x = 2.975330, f(x) = 0.000609\n",
      "Decayed learning rate to 0.044305\n",
      "Iteration 82: x = 2.975553, f(x) = 0.000598\n",
      "Decayed learning rate to 0.043862\n",
      "Iteration 83: x = 2.975774, f(x) = 0.000587\n",
      "Decayed learning rate to 0.043423\n",
      "Iteration 84: x = 2.975992, f(x) = 0.000576\n",
      "Decayed learning rate to 0.042989\n",
      "Iteration 85: x = 2.976208, f(x) = 0.000566\n",
      "Decayed learning rate to 0.042559\n",
      "Iteration 86: x = 2.976422, f(x) = 0.000556\n",
      "Decayed learning rate to 0.042133\n",
      "Iteration 87: x = 2.976634, f(x) = 0.000546\n",
      "Decayed learning rate to 0.041712\n",
      "Iteration 88: x = 2.976843, f(x) = 0.000536\n",
      "Decayed learning rate to 0.041295\n",
      "Iteration 89: x = 2.977050, f(x) = 0.000527\n",
      "Decayed learning rate to 0.040882\n",
      "Iteration 90: x = 2.977256, f(x) = 0.000517\n",
      "Decayed learning rate to 0.040473\n",
      "Iteration 91: x = 2.977459, f(x) = 0.000508\n",
      "Decayed learning rate to 0.040068\n",
      "Iteration 92: x = 2.977660, f(x) = 0.000499\n",
      "Decayed learning rate to 0.039668\n",
      "Iteration 93: x = 2.977858, f(x) = 0.000490\n",
      "Decayed learning rate to 0.039271\n",
      "Iteration 94: x = 2.978055, f(x) = 0.000482\n",
      "Decayed learning rate to 0.038878\n",
      "Iteration 95: x = 2.978250, f(x) = 0.000473\n",
      "Decayed learning rate to 0.038490\n",
      "Iteration 96: x = 2.978443, f(x) = 0.000465\n",
      "Decayed learning rate to 0.038105\n",
      "Iteration 97: x = 2.978634, f(x) = 0.000457\n",
      "Decayed learning rate to 0.037724\n",
      "Iteration 98: x = 2.978823, f(x) = 0.000448\n",
      "Decayed learning rate to 0.037346\n",
      "Iteration 99: x = 2.979010, f(x) = 0.000441\n",
      "Decayed learning rate to 0.036973\n",
      "Iteration 100: x = 2.979195, f(x) = 0.000433\n",
      "Decayed learning rate to 0.036603\n",
      "Iteration 101: x = 2.979378, f(x) = 0.000425\n",
      "Decayed learning rate to 0.036237\n",
      "Iteration 102: x = 2.979560, f(x) = 0.000418\n",
      "Decayed learning rate to 0.035875\n",
      "Iteration 103: x = 2.979739, f(x) = 0.000411\n",
      "Decayed learning rate to 0.035516\n",
      "Iteration 104: x = 2.979917, f(x) = 0.000403\n",
      "Decayed learning rate to 0.035161\n",
      "Iteration 105: x = 2.980093, f(x) = 0.000396\n",
      "Decayed learning rate to 0.034809\n",
      "Iteration 106: x = 2.980267, f(x) = 0.000389\n",
      "Decayed learning rate to 0.034461\n",
      "Iteration 107: x = 2.980440, f(x) = 0.000383\n",
      "Decayed learning rate to 0.034117\n",
      "Iteration 108: x = 2.980610, f(x) = 0.000376\n",
      "Decayed learning rate to 0.033775\n",
      "Iteration 109: x = 2.980779, f(x) = 0.000369\n",
      "Decayed learning rate to 0.033438\n",
      "Iteration 110: x = 2.980947, f(x) = 0.000363\n",
      "Decayed learning rate to 0.033103\n",
      "Iteration 111: x = 2.981112, f(x) = 0.000357\n",
      "Decayed learning rate to 0.032772\n",
      "Iteration 112: x = 2.981276, f(x) = 0.000351\n",
      "Decayed learning rate to 0.032445\n",
      "Iteration 113: x = 2.981439, f(x) = 0.000345\n",
      "Decayed learning rate to 0.032120\n",
      "Iteration 114: x = 2.981599, f(x) = 0.000339\n",
      "Decayed learning rate to 0.031799\n",
      "Iteration 115: x = 2.981758, f(x) = 0.000333\n",
      "Decayed learning rate to 0.031481\n",
      "Iteration 116: x = 2.981916, f(x) = 0.000327\n",
      "Decayed learning rate to 0.031166\n",
      "Iteration 117: x = 2.982072, f(x) = 0.000321\n",
      "Decayed learning rate to 0.030854\n",
      "Iteration 118: x = 2.982226, f(x) = 0.000316\n",
      "Decayed learning rate to 0.030546\n",
      "Iteration 119: x = 2.982379, f(x) = 0.000311\n",
      "Decayed learning rate to 0.030240\n",
      "Iteration 120: x = 2.982530, f(x) = 0.000305\n",
      "Decayed learning rate to 0.029938\n",
      "Iteration 121: x = 2.982680, f(x) = 0.000300\n",
      "Decayed learning rate to 0.029639\n",
      "Iteration 122: x = 2.982828, f(x) = 0.000295\n",
      "Decayed learning rate to 0.029342\n",
      "Iteration 123: x = 2.982975, f(x) = 0.000290\n",
      "Decayed learning rate to 0.029049\n",
      "Iteration 124: x = 2.983120, f(x) = 0.000285\n",
      "Decayed learning rate to 0.028758\n",
      "Iteration 125: x = 2.983264, f(x) = 0.000280\n",
      "Decayed learning rate to 0.028471\n",
      "Iteration 126: x = 2.983406, f(x) = 0.000275\n",
      "Decayed learning rate to 0.028186\n",
      "Iteration 127: x = 2.983547, f(x) = 0.000271\n",
      "Decayed learning rate to 0.027904\n",
      "Iteration 128: x = 2.983687, f(x) = 0.000266\n",
      "Decayed learning rate to 0.027625\n",
      "Iteration 129: x = 2.983825, f(x) = 0.000262\n",
      "Decayed learning rate to 0.027349\n",
      "Iteration 130: x = 2.983962, f(x) = 0.000257\n",
      "Decayed learning rate to 0.027075\n",
      "Iteration 131: x = 2.984097, f(x) = 0.000253\n",
      "Decayed learning rate to 0.026805\n",
      "Iteration 132: x = 2.984231, f(x) = 0.000249\n",
      "Decayed learning rate to 0.026537\n",
      "Iteration 133: x = 2.984364, f(x) = 0.000244\n",
      "Decayed learning rate to 0.026271\n",
      "Iteration 134: x = 2.984495, f(x) = 0.000240\n",
      "Decayed learning rate to 0.026009\n",
      "Iteration 135: x = 2.984625, f(x) = 0.000236\n",
      "Decayed learning rate to 0.025748\n",
      "Iteration 136: x = 2.984754, f(x) = 0.000232\n",
      "Decayed learning rate to 0.025491\n",
      "Iteration 137: x = 2.984882, f(x) = 0.000229\n",
      "Decayed learning rate to 0.025236\n",
      "Iteration 138: x = 2.985008, f(x) = 0.000225\n",
      "Decayed learning rate to 0.024984\n",
      "Iteration 139: x = 2.985133, f(x) = 0.000221\n",
      "Decayed learning rate to 0.024734\n",
      "Iteration 140: x = 2.985256, f(x) = 0.000217\n",
      "Decayed learning rate to 0.024487\n",
      "Iteration 141: x = 2.985379, f(x) = 0.000214\n",
      "Decayed learning rate to 0.024242\n",
      "Iteration 142: x = 2.985500, f(x) = 0.000210\n",
      "Decayed learning rate to 0.023999\n",
      "Iteration 143: x = 2.985620, f(x) = 0.000207\n",
      "Decayed learning rate to 0.023759\n",
      "Iteration 144: x = 2.985739, f(x) = 0.000203\n",
      "Decayed learning rate to 0.023522\n",
      "Iteration 145: x = 2.985857, f(x) = 0.000200\n",
      "Decayed learning rate to 0.023286\n",
      "Iteration 146: x = 2.985973, f(x) = 0.000197\n",
      "Decayed learning rate to 0.023054\n",
      "Iteration 147: x = 2.986088, f(x) = 0.000194\n",
      "Decayed learning rate to 0.022823\n",
      "Iteration 148: x = 2.986202, f(x) = 0.000190\n",
      "Decayed learning rate to 0.022595\n",
      "Iteration 149: x = 2.986315, f(x) = 0.000187\n",
      "Decayed learning rate to 0.022369\n",
      "Iteration 150: x = 2.986427, f(x) = 0.000184\n",
      "Decayed learning rate to 0.022145\n",
      "Iteration 151: x = 2.986538, f(x) = 0.000181\n",
      "Decayed learning rate to 0.021924\n",
      "Iteration 152: x = 2.986648, f(x) = 0.000178\n",
      "Decayed learning rate to 0.021704\n",
      "Iteration 153: x = 2.986756, f(x) = 0.000175\n",
      "Decayed learning rate to 0.021487\n",
      "Iteration 154: x = 2.986864, f(x) = 0.000173\n",
      "Decayed learning rate to 0.021273\n",
      "Iteration 155: x = 2.986970, f(x) = 0.000170\n",
      "Decayed learning rate to 0.021060\n",
      "Iteration 156: x = 2.987075, f(x) = 0.000167\n",
      "Decayed learning rate to 0.020849\n",
      "Iteration 157: x = 2.987179, f(x) = 0.000164\n",
      "Decayed learning rate to 0.020641\n",
      "Iteration 158: x = 2.987283, f(x) = 0.000162\n",
      "Decayed learning rate to 0.020434\n",
      "Iteration 159: x = 2.987385, f(x) = 0.000159\n",
      "Decayed learning rate to 0.020230\n",
      "Iteration 160: x = 2.987486, f(x) = 0.000157\n",
      "Decayed learning rate to 0.020028\n",
      "Iteration 161: x = 2.987586, f(x) = 0.000154\n",
      "Decayed learning rate to 0.019827\n",
      "Iteration 162: x = 2.987685, f(x) = 0.000152\n",
      "Decayed learning rate to 0.019629\n",
      "Iteration 163: x = 2.987783, f(x) = 0.000149\n",
      "Decayed learning rate to 0.019433\n",
      "Iteration 164: x = 2.987881, f(x) = 0.000147\n",
      "Decayed learning rate to 0.019239\n",
      "Iteration 165: x = 2.987977, f(x) = 0.000145\n",
      "Decayed learning rate to 0.019046\n",
      "Iteration 166: x = 2.988072, f(x) = 0.000142\n",
      "Decayed learning rate to 0.018856\n",
      "Iteration 167: x = 2.988166, f(x) = 0.000140\n",
      "Decayed learning rate to 0.018667\n",
      "Iteration 168: x = 2.988260, f(x) = 0.000138\n",
      "Decayed learning rate to 0.018480\n",
      "Iteration 169: x = 2.988352, f(x) = 0.000136\n",
      "Decayed learning rate to 0.018296\n",
      "Iteration 170: x = 2.988444, f(x) = 0.000134\n",
      "Decayed learning rate to 0.018113\n",
      "Iteration 171: x = 2.988534, f(x) = 0.000131\n",
      "Decayed learning rate to 0.017932\n",
      "Iteration 172: x = 2.988624, f(x) = 0.000129\n",
      "Decayed learning rate to 0.017752\n",
      "Iteration 173: x = 2.988713, f(x) = 0.000127\n",
      "Decayed learning rate to 0.017575\n",
      "Iteration 174: x = 2.988800, f(x) = 0.000125\n",
      "Decayed learning rate to 0.017399\n",
      "Iteration 175: x = 2.988887, f(x) = 0.000123\n",
      "Decayed learning rate to 0.017225\n",
      "Iteration 176: x = 2.988974, f(x) = 0.000122\n",
      "Decayed learning rate to 0.017053\n",
      "Iteration 177: x = 2.989059, f(x) = 0.000120\n",
      "Decayed learning rate to 0.016882\n",
      "Iteration 178: x = 2.989143, f(x) = 0.000118\n",
      "Decayed learning rate to 0.016713\n",
      "Iteration 179: x = 2.989227, f(x) = 0.000116\n",
      "Decayed learning rate to 0.016546\n",
      "Iteration 180: x = 2.989310, f(x) = 0.000114\n",
      "Decayed learning rate to 0.016381\n",
      "Iteration 181: x = 2.989391, f(x) = 0.000113\n",
      "Decayed learning rate to 0.016217\n",
      "Iteration 182: x = 2.989473, f(x) = 0.000111\n",
      "Decayed learning rate to 0.016055\n",
      "Iteration 183: x = 2.989553, f(x) = 0.000109\n",
      "Decayed learning rate to 0.015894\n",
      "Iteration 184: x = 2.989632, f(x) = 0.000107\n",
      "Decayed learning rate to 0.015735\n",
      "Iteration 185: x = 2.989711, f(x) = 0.000106\n",
      "Decayed learning rate to 0.015578\n",
      "Iteration 186: x = 2.989789, f(x) = 0.000104\n",
      "Decayed learning rate to 0.015422\n",
      "Iteration 187: x = 2.989866, f(x) = 0.000103\n",
      "Decayed learning rate to 0.015268\n",
      "Iteration 188: x = 2.989942, f(x) = 0.000101\n",
      "Decayed learning rate to 0.015115\n",
      "Iteration 189: x = 2.990018, f(x) = 0.000100\n",
      "Decayed learning rate to 0.014964\n",
      "Iteration 190: x = 2.990093, f(x) = 0.000098\n",
      "Decayed learning rate to 0.014814\n",
      "Iteration 191: x = 2.990167, f(x) = 0.000097\n",
      "Decayed learning rate to 0.014666\n",
      "Iteration 192: x = 2.990240, f(x) = 0.000095\n",
      "Decayed learning rate to 0.014520\n",
      "Iteration 193: x = 2.990313, f(x) = 0.000094\n",
      "Decayed learning rate to 0.014374\n",
      "Iteration 194: x = 2.990385, f(x) = 0.000092\n",
      "Decayed learning rate to 0.014231\n",
      "Iteration 195: x = 2.990456, f(x) = 0.000091\n",
      "Decayed learning rate to 0.014088\n",
      "Iteration 196: x = 2.990526, f(x) = 0.000090\n",
      "Decayed learning rate to 0.013948\n",
      "Iteration 197: x = 2.990596, f(x) = 0.000088\n",
      "Decayed learning rate to 0.013808\n",
      "Iteration 198: x = 2.990665, f(x) = 0.000087\n",
      "Decayed learning rate to 0.013670\n",
      "Iteration 199: x = 2.990733, f(x) = 0.000086\n",
      "Decayed learning rate to 0.013533\n",
      "Iteration 200: x = 2.990801, f(x) = 0.000085\n",
      "Decayed learning rate to 0.013398\n",
      "Iteration 201: x = 2.990868, f(x) = 0.000083\n",
      "Decayed learning rate to 0.013264\n",
      "Iteration 202: x = 2.990934, f(x) = 0.000082\n",
      "Decayed learning rate to 0.013131\n",
      "Iteration 203: x = 2.991000, f(x) = 0.000081\n",
      "Decayed learning rate to 0.013000\n",
      "Iteration 204: x = 2.991065, f(x) = 0.000080\n",
      "Decayed learning rate to 0.012870\n",
      "Iteration 205: x = 2.991129, f(x) = 0.000079\n",
      "Decayed learning rate to 0.012741\n",
      "Iteration 206: x = 2.991193, f(x) = 0.000078\n",
      "Decayed learning rate to 0.012614\n",
      "Iteration 207: x = 2.991256, f(x) = 0.000076\n",
      "Decayed learning rate to 0.012488\n",
      "Iteration 208: x = 2.991319, f(x) = 0.000075\n",
      "Decayed learning rate to 0.012363\n",
      "Iteration 209: x = 2.991380, f(x) = 0.000074\n",
      "Decayed learning rate to 0.012239\n",
      "Iteration 210: x = 2.991442, f(x) = 0.000073\n",
      "Decayed learning rate to 0.012117\n",
      "Iteration 211: x = 2.991502, f(x) = 0.000072\n",
      "Decayed learning rate to 0.011996\n",
      "Iteration 212: x = 2.991562, f(x) = 0.000071\n",
      "Decayed learning rate to 0.011876\n",
      "Iteration 213: x = 2.991621, f(x) = 0.000070\n",
      "Converged after 213 iterations.\n",
      "Optimal x: 2.991621\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example function: y = (x - 3)^2\n",
    "def function_to_minimize(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "# Gradient approximation using finite differences\n",
    "def approximate_gradient(f, x, epsilon=learning_rate):\n",
    "    return (f(x + epsilon) - f(x - epsilon)) / (2 * epsilon)\n",
    "\n",
    "def gradient_descent(starting_point, learning_rate, iterations, tolerance=1e-6, decay_rate=0.9, patience=10):\n",
    "    x = starting_point\n",
    "    prev_loss = function_to_minimize(x)\n",
    "    no_improve_counter = 0  # Tracks improvement\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # grad = approximate_gradient(function_to_minimize, x)  # Approximate the gradient\n",
    "        grad = (function_to_minimize(x+learning_rate) - function_to_minimize(x)) / (learning_rate)\n",
    "        x = x - learning_rate * grad                           # Update the parameter\n",
    "        \n",
    "        current_loss = function_to_minimize(x)\n",
    "        print(f\"Iteration {i+1}: x = {x:.6f}, f(x) = {current_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if abs(current_loss - prev_loss) < tolerance:\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            break\n",
    "        \n",
    "        # # Learning rate decay if no improvement\n",
    "        # if current_loss >= prev_loss:\n",
    "        #     no_improve_counter += 1\n",
    "        # else:\n",
    "        #     no_improve_counter = 0  # Reset counter if there is improvement\n",
    "        \n",
    "        # if no_improve_counter >= patience:\n",
    "        if 1:\n",
    "            learning_rate *= decay_rate  # Decay learning rate\n",
    "            print(f\"Decayed learning rate to {learning_rate:.6f}\")\n",
    "            no_improve_counter = 0  # Reset counter after decaying learning rate\n",
    "        \n",
    "        prev_loss = current_loss\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Parameters\n",
    "starting_point = 0.0  # Initial guess\n",
    "learning_rate = 0.1   # Initial learning rate\n",
    "iterations = 2000     # Max number of updates\n",
    "tolerance = 1e-6      # Convergence tolerance\n",
    "decay_rate = 0.99     # Learning rate decay multiplier\n",
    "patience = 10         # Number of iterations with no improvement before decaying\n",
    "\n",
    "# Run gradient descent\n",
    "optimal_x = gradient_descent(starting_point, learning_rate, iterations, tolerance, decay_rate, patience)\n",
    "print(f\"Optimal x: {optimal_x:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mcts4py310-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
